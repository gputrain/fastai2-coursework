{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f11c567",
   "metadata": {},
   "source": [
    "# UrbanSounds8K sound classification visual approach comparison -  Linear, Log Spectrograms vs Mel Spectrograms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad31153",
   "metadata": {},
   "source": [
    "## About the UrbanSounds8K dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5928c1b",
   "metadata": {},
   "source": [
    "Urban Sounds is a dataset of 8732 labeled sounds of less than 4 seconds each from 10 classes. Dataset for [UrbanSounds8K](https://urbansounddataset.weebly.com/urbansound8k.html) contains these 10 classes:\n",
    "\n",
    "1.  air_conditioner\n",
    "2.  car_horn\n",
    "3.  children_playing\n",
    "4.  dog_bark\n",
    "5.  drilling\n",
    "6.  engine_idling\n",
    "7.  gun_shot\n",
    "8.  jackhammer\n",
    "9.  siren\n",
    "10. street_music\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccd0308",
   "metadata": {},
   "source": [
    "## Background and objectives for this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9643b3",
   "metadata": {},
   "source": [
    "[Research with this dataset as of 2019](https://www.researchgate.net/publication/335862311_Evaluation_of_Classical_Machine_Learning_Techniques_towards_Urban_Sound_Recognition_on_Embedded_Systems) and optimized ML approaches as of late 2019 had classification accuracy at 74% with a k-nearest neighbours (KNN) algorithm. A deep learning neural network trained from scratch obtained accuracy at 76% accuracy.\n",
    "\n",
    "\n",
    "![Accuracy metrics](https://www.researchgate.net/profile/Bruno-Silva-172/publication/335862311/figure/fig2/AS:804132151652353@1568731453277/Achieved-accuracy-of-the-classifiers-with-their-default-and-optimized-configuration.png \"research\")\n",
    "\n",
    "*(accuracy metrics for research article)*\n",
    "\n",
    "The state-of-the-art methods for audio classification approach  this problem as an image classification task. For such image classification problems from audio samples, three common transformation approaches are:\n",
    "\n",
    "- 1. Linear Spectrograms\n",
    "- 2. Log Spectrograms\n",
    "- 3. Mel Spectrograms\n",
    "\n",
    "You can learn more about these three transformations in [Scott Duda's article](https://scottmduda.medium.com/urban-environmental-audio-classification-using-mel-spectrograms-706ee6f8dcc1) and [Ketan Doshi's writing](https://towardsdatascience.com/audio-deep-learning-made-simple-part-2-why-mel-spectrograms-perform-better-aad889a93505), reasoning why Mel Spectrograms perform better in general for visual transformations of audio files.\n",
    "\n",
    "This notebook will test these three transforms on this Urban Sounds 8K dataset and how they perform with a pre-trained vision-based model (Resnet-34) leveraging Fastaiv2. Subsequently will see if other pre-trained models can improve upon the Resnet-34 pre-trained model on these results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb0dc9f",
   "metadata": {},
   "source": [
    "This notebook converts these sounds to a spectrogram then uses FastAI2 code base to classify these sounds. Code and approach in this notebook "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38be5bcc",
   "metadata": {},
   "source": [
    "### Setup for AWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3d0b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#One time installs  - On AWS useconda_pytorch_p38 environment and add using ml.p3.2xlarge for this notebook\n",
    "!pip install librosa\n",
    "!pip install fastbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ddef24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#all the one time imports for this nb\n",
    "import pandas as pd\n",
    "\n",
    "from fastai.vision.all import *\n",
    "from fastai.data.all import *\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import specgram\n",
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import os\n",
    "import random\n",
    "import IPython\n",
    "from tqdm import tqdm\n",
    "\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca2e0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One time download files to local S3 folder\n",
    "# !wget https://goo.gl/8hY5ER  #download\n",
    "# !tar xf 8hY5ER #unpack tar file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76cc9ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('UrbanSound8K/metadata/UrbanSound8K.csv')  #classification information across folds as provided from Urbansounds\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af9993e",
   "metadata": {},
   "source": [
    "##### Class distribution across the sound types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ff8cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('class').classID.count().sort_values(ascending=False).plot.bar()\n",
    "plt.ylabel('count')\n",
    "plt.title('Class distribution in the dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812b3fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['fold']).classID.count().sort_values(ascending=False).plot.bar()\n",
    "plt.ylabel('Files in each fold')\n",
    "plt.title('Files in each fold')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090cd569",
   "metadata": {},
   "source": [
    "##### Inspect the files - audio and single tranform of a audio file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d981361e",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_file= 'UrbanSound8K/audio/fold5/100032-3-0-0.wav'   #dog bark in fold 5\n",
    "\n",
    "IPython.display.Audio(audio_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ac7bf0",
   "metadata": {},
   "source": [
    "##### Linear Spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3a9ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples, sample_rate = librosa.load(audio_file)\n",
    "Ydb = librosa.amplitude_to_db(librosa.stft(samples), ref=sample_rate)\n",
    "plt.figure(figsize=(18, 6))\n",
    "librosa.display.specshow(Ydb, sr=sample_rate, x_axis='time', y_axis='linear')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4637afa",
   "metadata": {},
   "source": [
    "##### Log Spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ab3cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18, 6))\n",
    "librosa.display.specshow(Ydb, sr=sample_rate, x_axis='time', y_axis='log')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54bcd1da",
   "metadata": {},
   "source": [
    "##### Mel Spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24dc6a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "S = librosa.feature.melspectrogram(y=samples, sr=sample_rate)\n",
    "Sdb = librosa.power_to_db(S, ref=np.max)\n",
    "plt.figure(figsize=(18, 6))\n",
    "librosa.display.specshow(Sdb, sr=sample_rate, x_axis='time', y_axis='mel')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12300f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_path = Path('UrbanSound8K/audio/')  # un zipped source audio files are in this location as wav files\n",
    "tranform_store_path = 'UrbanSoundTransforms/'  #destination folder for each transformed image state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4aca34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make initial folders once\n",
    "#os.mkdir(tranform_store_path)\n",
    "# os.mkdir(tranform_store_path +'linear_spectrogram')\n",
    "# os.mkdir(tranform_store_path +'log_spectrogram')\n",
    "# os.mkdir(tranform_store_path +'mel_spectrogram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749c3480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for fold in np.arange (1,11):\n",
    "#     print(f'Processing fold {fold}')\n",
    "#     try:\n",
    "#         os.mkdir(tranform_store_path+'linear_spectrogram/'+ str(fold))\n",
    "#         os.mkdir(tranform_store_path+'log_spectrogram/'+ str(fold))\n",
    "#         os.mkdir(tranform_store_path+'mel_spectrogram/'+str(fold))\n",
    "#     except:\n",
    "#         pass #Folder exists\n",
    "#     for audio_file in tqdm(list(Path(audio_path/f'fold{fold}').glob('*.wav'))):\n",
    "#         samples, sample_rate = librosa.load(audio_file)  #create onces with librosa\n",
    "        \n",
    "#         #plot for linear spectrogram - without axis, tight \n",
    "        \n",
    "#         fig = plt.figure(figsize=[0.72,0.72])\n",
    "#         ax = fig.add_subplot(111)\n",
    "#         ax.axes.get_xaxis().set_visible(False)\n",
    "#         ax.axes.get_yaxis().set_visible(False)\n",
    "#         ax.set_frame_on(False)\n",
    "#         Ydb = librosa.amplitude_to_db(librosa.stft(samples), ref=sample_rate)\n",
    "#         LS = librosa.display.specshow(Ydb, sr=sample_rate, x_axis='time', y_axis='linear')\n",
    "#         filename  = tranform_store_path + 'linear_spectrogram/'+str(fold) +'/'+ str(audio_file).split('/')[-1:][0].replace('.wav','.png')\n",
    "#         plt.savefig(filename, dpi=400, bbox_inches='tight',pad_inches=0)\n",
    "#         plt.close('all')\n",
    "        \n",
    "#         #plot for log  spectrogram - without axis, tight \n",
    "#         fig = plt.figure(figsize=[0.72,0.72])\n",
    "#         ax = fig.add_subplot(111)\n",
    "#         ax.axes.get_xaxis().set_visible(False)\n",
    "#         ax.axes.get_yaxis().set_visible(False)\n",
    "#         ax.set_frame_on(False)\n",
    "#         LogS = librosa.display.specshow(Ydb, sr=sample_rate,x_axis='time', y_axis='log')\n",
    "#         filename  = tranform_store_path + 'log_spectrogram/'+str(fold) +'/'+ str(audio_file).split('/')[-1:][0].replace('.wav','.png')\n",
    "#         plt.savefig(filename, dpi=400, bbox_inches='tight',pad_inches=0)\n",
    "#         plt.close('all')\n",
    "        \n",
    "#         #plot for mel spectrogram - without axis, tight\n",
    "        \n",
    "#         fig = plt.figure(figsize=[0.72,0.72])\n",
    "#         ax = fig.add_subplot(111)\n",
    "#         ax.axes.get_xaxis().set_visible(False)\n",
    "#         ax.axes.get_yaxis().set_visible(False)\n",
    "#         ax.set_frame_on(False)\n",
    "#         melS = librosa.feature.melspectrogram(y=samples, sr=sample_rate)\n",
    "#         librosa.display.specshow(librosa.power_to_db(melS, ref=np.max))\n",
    "#         filename  = tranform_store_path + 'mel_spectrogram/'+str(fold) +'/'+ str(audio_file).split('/')[-1:][0].replace('.wav','.png')\n",
    "#         plt.savefig(filename, dpi=400, bbox_inches='tight',pad_inches=0)\n",
    "#         plt.close('all')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287832c0",
   "metadata": {},
   "source": [
    "##### Validate all files are transformed in destination folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cafe97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = ['linear_spectrogram/','log_spectrogram/','mel_spectrogram/']\n",
    "for transform in transforms:\n",
    "    count = 0\n",
    "    for fold in np.arange (1,11):\n",
    "        count += len(list(Path(tranform_store_path+transform+str(fold)).glob('*.png')))\n",
    "    print ('%s file count is %s'%(transform[:-1],count))\n",
    "    assert (len(df)==count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8c4b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = OrderedDict(sorted(df.set_index('classID').to_dict()['class'].items()))\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e6e2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(10,3, figsize=(16,16))\n",
    "for k,v in classes.items():\n",
    "    sample = df[df['class']==v].sample(1)\n",
    "    sample_fold = sample['fold'].values[0]\n",
    "    sample_file = sample['slice_file_name'].values[0].replace('wav','png')\n",
    "    t_counter=0\n",
    "    for transform in transforms:\n",
    "        img = plt.imread(tranform_store_path+transform+str(sample_fold)+'/'+sample_file)\n",
    "        ax[k][t_counter].imshow(img, aspect='equal')\n",
    "        ax[k][t_counter].set_title(v+' transformed with '+ transform[:-1])\n",
    "        ax[k][t_counter].title.set_size(10)\n",
    "        ax[k][t_counter].set_axis_off()\n",
    "        \n",
    "        t_counter+=1\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89d308c",
   "metadata": {},
   "source": [
    "##### Fast AI classification of these spectrograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ab9660",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['fname'] = df[['slice_file_name','fold']].apply (lambda x: str(x['slice_file_name'][:-4])+'.png'.strip(),axis=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6720d30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dict = dict(zip(df.fname,df['class']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98980c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_func(f_name):\n",
    "    f_name = str(f_name).split('/')[-1:][0]\n",
    "    return my_dict[f_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921c5d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_folds = list(np.arange(1,11))\n",
    "all_folders = [str(i) for i in all_folds]\n",
    "all_folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6881a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef1abe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for transform in transforms:\n",
    "    all_files = get_image_files(path=tranform_store_path+transform,recurse=True, folders =all_folders )\n",
    "    \n",
    "    for test_folder in all_folds:\n",
    "        \n",
    "        dblock = DataBlock(blocks=(ImageBlock,CategoryBlock),\n",
    "                   get_y     = label_func,\n",
    "                   \n",
    "                   splitter  = FuncSplitter(lambda s: Path(s).parent.name==str(test_folder)),\n",
    "                   \n",
    "                  )\n",
    "        dl = dblock.dataloaders(all_files)\n",
    "        \n",
    "        print ('Train has {0} images and test has {1} images. Test is on folder {2} of transform type {3}.' .format(len(dl.train_ds),len(dl.valid_ds),test_folder,transform[:-1]))\n",
    "        learn = vision_learner(dl, resnet34, metrics=accuracy)\n",
    "        learn.fine_tune(3)\n",
    "        r = learn.validate()\n",
    "        results.at[test_folder,transform[:-1]] = r[1]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b10ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(3,lr= (learn_rate[0]+learn_rate[1])/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024c82f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform ='mel_spectrogram/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b5f464",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = get_image_files(path=tranform_store_path+transform,recurse=True, folders =all_folders )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a00c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "dblock = DataBlock(blocks=(ImageBlock,CategoryBlock),\n",
    "                   get_y     = label_func,\n",
    "                   \n",
    "                   splitter  = FuncSplitter(lambda s: Path(s).parent.name==str(test_folder)),\n",
    "                   \n",
    "                  )\n",
    "dl = dblock.dataloaders(all_files)\n",
    "\n",
    "print ('Train has {0} images and test has {1} images. Test is on folder {2} of transform type {3}.' .format(len(dl.train_ds),len(dl.valid_ds),test_folder,transform[:-1]))\n",
    "learn = vision_learner(dl, resnet34, metrics=accuracy)\n",
    "learn.fine_tune(3)\n",
    "r = learn.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea0dc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "r[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0cb76b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p38",
   "language": "python",
   "name": "conda_pytorch_p38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
